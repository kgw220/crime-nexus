{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed91550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an experimental notebook to walkthrough the process of getting relevant weather, crime, and\n",
    "census data for the city of Philadelphia, PA.\n",
    "\"\"\"\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def fetch_crime_data(table: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads Philadelphia crime data from OpenDataPhilly in the specified date range.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    table: str\n",
    "        The name of the table in the OpenDataPhilly database.\n",
    "    start_date: str\n",
    "        The start date for the data in 'YYYY-MM-DD' format.\n",
    "    end_date: str\n",
    "        The end date for the data in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the crime data with relevant columns.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting crime data from {start_date} to {end_date}...\")\n",
    "\n",
    "    # Simple SQL query to get the data needed from the OpenDataPhilly database\n",
    "    # Subsetting to the columns we need, some are left out because I think they are irrelevant\n",
    "\n",
    "    # Variables Selected:\n",
    "    # - dc_dist:         The police district where the incident occurred.\n",
    "    # - psa:             The Police Service Area, a smaller geographic subdivision of a district.\n",
    "    # - dispatch_date:   The date the call for service was dispatched (YYYY-MM-DD).\n",
    "    # - dispatch_time:   The time the call for service was dispatched (HH:MI:SS).\n",
    "    # - hour:            The hour of the day the call was dispatched (0-23).\n",
    "    # - text_general_code: The text description for the type of incident (e.g., \"Theft,\" \"Assault\").\n",
    "    # - location_block:  The street address of the incident, anonymized to the block level.\n",
    "    # - lat:             The latitude coordinate for the incident location (aliased from point_y).\n",
    "    # - lon:             The longitude coordinate for the incident location (aliased from point_x).\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT dc_dist, psa, dispatch_date, dispatch_time, hour, text_general_code, location_block,\n",
    "        point_y as lat, point_x as lon\n",
    "        FROM {table}\n",
    "        WHERE dispatch_date >= '{start_date}' AND dispatch_date <= '{end_date}'\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the OpenDataPhilly API and get the data\n",
    "    response = requests.get(CARTO_URL, params={\"q\": query})\n",
    "    response.raise_for_status()\n",
    "    data = response.json().get(\"rows\", [])\n",
    "    crime_df = pd.DataFrame(data)\n",
    "\n",
    "    return crime_df\n",
    "\n",
    "\n",
    "def clean_crime_data(crime_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the crime data to prepare for analysis. This involves steps like one-hot encoding,\n",
    "    removing NAs, renaming columns, and creating new features.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    crime_df: pd.DataFrame\n",
    "        The DataFrame containing the raw crime data.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A cleaned DataFrame with relevant columns and features for analysis.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning crime data...\")\n",
    "\n",
    "    # Drop rows with missing latitude, longitude, or police service area (psa)\n",
    "    crime_df = crime_df.dropna(subset=[\"lat\", \"lon\", \"psa\"])\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    crime_rename_map = {\n",
    "        \"dc_dist\": \"police_district\",\n",
    "        \"psa\": \"police_service_area\",\n",
    "        \"text_general_code\": \"crime_type\",\n",
    "        \"location_block\": \"address_block\",\n",
    "    }\n",
    "    crime_df = crime_df.rename(columns=crime_rename_map)\n",
    "\n",
    "    # Convert police district and service area to dummy variables since they are categorial\n",
    "    crime_df[\"police_district\"] = crime_df[\"police_district\"].astype(str)\n",
    "    crime_df[\"police_service_area\"] = crime_df[\"police_service_area\"].astype(str)\n",
    "    district_dummies = pd.get_dummies(crime_df[\"police_district\"], prefix=\"district\")\n",
    "    psa_dummies = pd.get_dummies(crime_df[\"police_service_area\"], prefix=\"psa\")\n",
    "    crime_df = crime_df.join(district_dummies)\n",
    "    crime_df = crime_df.join(psa_dummies)\n",
    "    crime_df = crime_df.drop(columns=[\"police_district\", \"police_service_area\"])\n",
    "\n",
    "    # Create dummy columns for every unique crime type\n",
    "    dummies = pd.get_dummies(crime_df[\"crime_type\"], prefix=\"crime\")\n",
    "    crime_df = crime_df.join(dummies)\n",
    "    crime_df = crime_df.drop(columns=[\"crime_type\"])\n",
    "\n",
    "    # Convert relevant columns to datetime\n",
    "    crime_df[\"dispatch_date_dt\"] = pd.to_datetime(crime_df[\"dispatch_date\"]).dt.date\n",
    "    crime_df[\"dispatch_date\"] = pd.to_datetime(crime_df[\"dispatch_date\"])\n",
    "    crime_df[\"dispatch_time\"] = pd.to_datetime(crime_df[\"dispatch_time\"])\n",
    "\n",
    "    # Extract hour from dispatch_time; while the original dataframe had an hour column, it had missing\n",
    "    # values, so I decided to do it myself\n",
    "    crime_df[\"hour\"] = crime_df[\"dispatch_time\"].dt.hour\n",
    "\n",
    "    # Define hour/month/day of week cylic features\n",
    "    crime_df[\"hour_sin\"] = np.sin(2 * np.pi * crime_df[\"hour\"] / 24.0)\n",
    "    crime_df[\"hour_cos\"] = np.cos(2 * np.pi * crime_df[\"hour\"] / 24.0)\n",
    "    crime_df[\"month\"] = crime_df[\"dispatch_date\"].dt.month\n",
    "    crime_df[\"month_sin\"] = np.sin(2 * np.pi * crime_df[\"month\"] / 12.0)\n",
    "    crime_df[\"month_cos\"] = np.cos(2 * np.pi * crime_df[\"month\"] / 12.0)\n",
    "    crime_df[\"day_of_week\"] = crime_df[\"dispatch_date\"].dt.dayofweek\n",
    "    crime_df[\"day_of_week_sin\"] = np.sin(2 * np.pi * crime_df[\"day_of_week\"] / 7.0)\n",
    "    crime_df[\"day_of_week_cos\"] = np.cos(2 * np.pi * crime_df[\"day_of_week\"] / 7.0)\n",
    "    crime_df = crime_df.drop(columns=[\"hour\", \"month\", \"day_of_week\"])\n",
    "\n",
    "    # Print min and max dispatch date for sanity check to ensure the data is within the expected range\n",
    "    min_date = crime_df[\"dispatch_date\"].min()\n",
    "    max_date = crime_df[\"dispatch_date\"].max()\n",
    "    print(f\"Min dispatch_date: {min_date}\")\n",
    "    print(f\"Max dispatch_date: {max_date}\")\n",
    "\n",
    "    return crime_df\n",
    "\n",
    "\n",
    "def fetch_weather_data(\n",
    "    station_id: str, start_date: str, end_date: str, token: str, max_retries: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads daily weather data from NOAA for a single station.\n",
    "    This function is designed to be called for a limited date range (e.g., one year), to avoid\n",
    "    overwhelming the API with a single large request.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    station_id: str\n",
    "        The NOAA station ID for the weather station (e.g., \"GHCND:USW00013739\" for Philadelphia Intl Airport).\n",
    "    start_date: str\n",
    "        The start date for the data in 'YYYY-MM-DD' format.\n",
    "    end_date: str\n",
    "        The end date for the data in 'YYYY-MM-DD' format.\n",
    "    token: str\n",
    "        Your NOAA API token for authentication.\n",
    "    max_retries: int\n",
    "        The maximum number of retries for failed requests (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the weather data with relevant columns.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching weather from {start_date} to {end_date}...\")\n",
    "\n",
    "    headers = {\"token\": token}\n",
    "    all_results = []\n",
    "    offset = 1\n",
    "\n",
    "    # Get weather data from NOAA API\n",
    "    # Variables Selected:\n",
    "    # TMAX: Maximum Temperature\n",
    "    # TMIN: Minimum Temperature\n",
    "    # PRCP: Precipitation\n",
    "    # AWND: Average Wind Speed\n",
    "    # SNOW: Snowfall\n",
    "    # SNWD: Snow Depth\n",
    "    # NOTE: more can be added as per the documentation:\n",
    "    # https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf\n",
    "    while True:\n",
    "        params = {\n",
    "            \"datasetid\": \"GHCND\",\n",
    "            \"stationid\": station_id,\n",
    "            \"startdate\": start_date,\n",
    "            \"enddate\": end_date,\n",
    "            \"units\": \"standard\",\n",
    "            \"limit\": 1000,\n",
    "            \"offset\": offset,\n",
    "            \"datatypeid\": [\"TMAX\", \"TMIN\", \"PRCP\", \"AWND\", \"SNOW\", \"SNWD\"],\n",
    "        }\n",
    "\n",
    "        # Retry logic for temporary server errors\n",
    "        response = None\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    WEATHER_URL, headers=headers, params=params, timeout=20\n",
    "                )\n",
    "                if response.status_code in [500, 502, 503, 504]:\n",
    "                    raise requests.exceptions.HTTPError(\n",
    "                        f\"{response.status_code} Server Error\"\n",
    "                    )\n",
    "                response.raise_for_status()\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                wait_time = 2**retries\n",
    "                print(f\"Warning: Request failed ({e}). Retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "        if not response or response.status_code != 200:\n",
    "            print(f\" Error: Failed to download data after {max_retries} retries.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        results = response.json().get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        # Process the results and append to the list\n",
    "        all_results.extend(results)\n",
    "        offset += 1000\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Format the results into a DataFrame and do some basic cleaning to help later on\n",
    "    df = pd.DataFrame(all_results)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df[\"date_dt\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "    weather_df = df.pivot_table(\n",
    "        index=\"date_dt\", columns=\"datatype\", values=\"value\"\n",
    "    ).reset_index()\n",
    "\n",
    "    return weather_df\n",
    "\n",
    "\n",
    "def clean_weather_data(weather_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the weather data to prepare for analysis. This involves renaming columns for now.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    weather_df: pd.DataFrame\n",
    "        The DataFrame containing the raw weather data.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A cleaned DataFrame with relevant columns and features for analysis.\n",
    "    \"\"\"\n",
    "    # Rename columns for clarity\n",
    "    weather_rename_map = {\n",
    "        \"AWND\": \"avg_wind_speed_mph\",\n",
    "        \"PRCP\": \"precipitation_inches\",\n",
    "        \"SNOW\": \"snowfall_inches\",\n",
    "        \"SNWD\": \"snow_depth_inches\",\n",
    "        \"TMAX\": \"max_temp_f\",\n",
    "        \"TMIN\": \"min_temp_f\",\n",
    "    }\n",
    "    weather_df = weather_df.rename(columns=weather_rename_map)\n",
    "\n",
    "    return weather_df\n",
    "\n",
    "\n",
    "def fetch_census_data(state_fips: str, county_fips: str, token: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads and processes ACS 5-Year data for all tracts in a county, including calculation of key\n",
    "    demographic rates.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    state_fips: str\n",
    "        The FIPS code for the state (e.g., \"42\" for Pennsylvania).\n",
    "    county_fips: str\n",
    "        The FIPS code for the county (e.g., \"101\" for Philadelphia County).\n",
    "    token: str\n",
    "        Your Census API token for authentication.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the census data with relevant columns and calculated rates.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Downloading census data for Philadelphia County (FIPS: {state_fips}{county_fips})...\"\n",
    "    )\n",
    "\n",
    "    # Variables Selected:\n",
    "    # NAME: Geographic Area Name\n",
    "    # B01003_001E: Total Population\n",
    "    # B19013_001E: Median Household Income\n",
    "    # B17001_002E: Poverty Count (total people below poverty line)\n",
    "    # B01002_001E: Median Age\n",
    "    # B25002_001E: Total Housing Units\n",
    "    # B25002_003E: Vacant Housing Units\n",
    "    # B25003_001E: Total Occupied Housing Units\n",
    "    # B25003_003E: Renter-Occupied Housing Units\n",
    "    # NOTE: could get more variables, as listed here:\n",
    "    # https://api.census.gov/data/2023/acs/acs5/variables.html\n",
    "    variables = \"NAME,B01003_001E,B19013_001E,B17001_002E,B01002_001E,B25002_001E,B25002_003E,B25003_001E,B25003_003E\"\n",
    "    params = {\n",
    "        \"get\": variables,\n",
    "        \"for\": \"tract:*\",\n",
    "        \"in\": f\"state:{state_fips} county:{county_fips}\",\n",
    "        \"key\": token,\n",
    "    }\n",
    "\n",
    "    # Make the request to the Census API and parse the response\n",
    "    response = requests.get(CENSUS_API_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    census_df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "    return census_df\n",
    "\n",
    "\n",
    "def clean_census_data(census_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the census data to prepare for analysis. This involves renaming columns for clarity and\n",
    "    creating new columns.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    census_df: pd.DataFrame\n",
    "        The DataFrame containing the raw census data.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A cleaned DataFrame with relevant columns and features for analysis.\n",
    "    \"\"\"\n",
    "    # Rename columns for clarity on what they represent\n",
    "    census_df = census_df.rename(\n",
    "        columns={\n",
    "            \"B01003_001E\": \"pop_total\",\n",
    "            \"B19013_001E\": \"income_median\",\n",
    "            \"B17001_002E\": \"poverty_total\",\n",
    "            \"B01002_001E\": \"median_age\",\n",
    "            \"B25002_001E\": \"total_housing_units\",\n",
    "            \"B25002_003E\": \"vacant_housing_units\",\n",
    "            \"B25003_001E\": \"total_occupied_units\",\n",
    "            \"B25003_003E\": \"renter_occupied_units\",\n",
    "            \"tract\": \"tract_fips_short\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create the full FIPS code for joining\n",
    "    census_df[\"tract_fips\"] = (\n",
    "        census_df[\"state\"] + census_df[\"county\"] + census_df[\"tract_fips_short\"]\n",
    "    )\n",
    "\n",
    "    # Convert all relevant columns to numeric, handling errors\n",
    "    numeric_cols = [\n",
    "        \"pop_total\",\n",
    "        \"income_median\",\n",
    "        \"poverty_total\",\n",
    "        \"median_age\",\n",
    "        \"total_housing_units\",\n",
    "        \"vacant_housing_units\",\n",
    "        \"total_occupied_units\",\n",
    "        \"renter_occupied_units\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        census_df[col] = pd.to_numeric(census_df[col], errors=\"coerce\")\n",
    "\n",
    "    # Calculate new features which are useful for the clustering task\n",
    "    # Replace zeros with NaN to prevent division errors\n",
    "    census_df[\"pop_total\"] = census_df[\"pop_total\"].replace(0, np.nan)\n",
    "    census_df[\"total_housing_units\"] = census_df[\"total_housing_units\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "    census_df[\"total_occupied_units\"] = census_df[\"total_occupied_units\"].replace(\n",
    "        0, np.nan\n",
    "    )\n",
    "    census_df[\"poverty_rate\"] = census_df[\"poverty_total\"] / census_df[\"pop_total\"]\n",
    "    census_df[\"vacancy_rate\"] = (\n",
    "        census_df[\"vacant_housing_units\"] / census_df[\"total_housing_units\"]\n",
    "    )\n",
    "    census_df[\"renter_occupancy_rate\"] = (\n",
    "        census_df[\"renter_occupied_units\"] / census_df[\"total_occupied_units\"]\n",
    "    )\n",
    "\n",
    "    # Replace any resulting NaN values (from division by zero) with 0\n",
    "    rate_cols = [\"poverty_rate\", \"vacancy_rate\", \"renter_occupancy_rate\"]\n",
    "    census_df[rate_cols] = census_df[rate_cols].fillna(0)\n",
    "\n",
    "    print(f\"Downloaded and processed census data for {len(census_df)} tracts.\")\n",
    "\n",
    "    # Return the final, clean set of columns\n",
    "    final_columns = [\n",
    "        \"tract_fips\",\n",
    "        \"pop_total\",\n",
    "        \"income_median\",\n",
    "        \"median_age\",\n",
    "        \"poverty_rate\",\n",
    "        \"vacancy_rate\",\n",
    "        \"renter_occupancy_rate\",\n",
    "    ]\n",
    "\n",
    "    return census_df[final_columns]\n",
    "\n",
    "\n",
    "def get_census_tracts():\n",
    "    \"\"\"\n",
    "    Fetches census tracts from OpenDataPhilly API and returns them as a GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    gpd.GeoDataFrame\n",
    "        A GeoDataFrame containing the census tracts with their geometries.\n",
    "    \"\"\"\n",
    "    print(\"Fetching census tracts from OpenDataPhilly API...\")\n",
    "\n",
    "    # Make the API call to get the GeoJSON data\n",
    "    response = requests.get(CENSUS_SHAPE_URL)\n",
    "\n",
    "    # This will raise an error if the request fails\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Load the GeoJSON response directly into a GeoDataFrame\n",
    "    gdf_tracts = gpd.GeoDataFrame.from_features(response.json()[\"features\"])\n",
    "\n",
    "    # Set the coordinate reference system, which is standard for web data\n",
    "    gdf_tracts = gdf_tracts.set_crs(\"EPSG:4326\")\n",
    "\n",
    "    return gdf_tracts\n",
    "\n",
    "\n",
    "def merge_crime_census(\n",
    "    crime_df: pd.DataFrame, census_tracts: gpd.GeoDataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges crime data with census data based on spatial join with census tracts.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    crime_df: pd.DataFrame\n",
    "        The DataFrame containing the cleaned crime data.\n",
    "    census_df: pd.DataFrame\n",
    "        The DataFrame containing the cleaned census data.\n",
    "    census_tracts: gpd.GeoDataFrame\n",
    "        The GeoDataFrame containing the census tracts geometries.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A merged DataFrame containing crime and census data.\n",
    "    \"\"\"\n",
    "    # Convert the crime DataFrame to a GeoDataFrame\n",
    "    print(\"Converting crime data to GeoDataFrame...\")\n",
    "    crime_gdf = gpd.GeoDataFrame(\n",
    "        crime_df,\n",
    "        geometry=gpd.points_from_xy(crime_df.lon, crime_df.lat),\n",
    "        crs=\"EPSG:4326\",  # Ensure CRS matches the census tract data\n",
    "    )\n",
    "\n",
    "    # Perform the spatial join\n",
    "    print(\"Performing spatial join to map crimes to census tracts...\")\n",
    "    # Note: The 'predicate' argument was renamed to 'op' in recent geopandas versions\n",
    "    try:\n",
    "        # For newer GeoPandas versions\n",
    "        final_crime_data = gpd.sjoin(crime_gdf, census_tracts, how=\"inner\", op=\"within\")\n",
    "    except TypeError:\n",
    "        # For older GeoPandas versions\n",
    "        final_crime_data = gpd.sjoin(\n",
    "            crime_gdf, census_tracts, how=\"inner\", predicate=\"within\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n Spatial join complete. Crime data now includes census tract info.\")\n",
    "\n",
    "    # Clean up the final DataFrame\n",
    "    final_crime_data = final_crime_data.drop(\n",
    "        columns=[\n",
    "            \"OBJECTID\",\n",
    "            \"STATEFP10\",\n",
    "            \"COUNTYFP10\",\n",
    "            \"TRACTCE10\",\n",
    "            \"NAME10\",\n",
    "            \"NAMELSAD10\",\n",
    "            \"MTFCC10\",\n",
    "            \"FUNCSTAT10\",\n",
    "            \"AWATER10\",\n",
    "            \"INTPTLAT10\",\n",
    "            \"INTPTLON10\",\n",
    "            \"LOGRECNO\",\n",
    "            \"index_right\",\n",
    "        ]\n",
    "    )\n",
    "    final_crime_map = {\n",
    "        \"GEOID10\": \"tract_id\",\n",
    "        \"ALAND10\": \"land_area_sq_meters\",\n",
    "    }\n",
    "    final_crime_data = final_crime_data.rename(columns=final_crime_map)\n",
    "\n",
    "    return final_crime_data\n",
    "\n",
    "\n",
    "def calculate_population_density(merged_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates population density for each census tract in the merged DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    merged_df: pd.DataFrame\n",
    "        The DataFrame containing the merged crime, weather, and census data.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame with an additional column for population density.\n",
    "    \"\"\"\n",
    "    print(\"Calculating population density...\")\n",
    "\n",
    "    # Convert land_area_sq_meters to square kilometers, replacing 0 with NaN to avoid errors\n",
    "    merged_df[\"area_sq_km\"] = merged_df[\"land_area_sq_meters\"] / SQ_METERS_PER_SQ_KM\n",
    "    merged_df[\"area_sq_km\"] = merged_df[\"area_sq_km\"].replace(0, np.nan)\n",
    "\n",
    "    # Calculate density and fill any missing results with 0\n",
    "    merged_df[\"pop_density_sq_km\"] = merged_df[\"pop_total\"] / merged_df[\"area_sq_km\"]\n",
    "    merged_df[\"pop_density_sq_km\"] = merged_df[\"pop_density_sq_km\"].fillna(0)\n",
    "\n",
    "    print(\"Population density calculated.\")\n",
    "\n",
    "    # Display the new column to verify the result\n",
    "    print(\n",
    "        merged_df[\n",
    "            [\"tract_id\", \"pop_total\", \"land_area_sq_meters\", \"pop_density_sq_km\"]\n",
    "        ].head()\n",
    "    )\n",
    "\n",
    "    # Drop the land_area_sq_meters column as it's no longer needed\n",
    "    merged_df = merged_df.drop(columns=[\"land_area_sq_meters\"])\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d3f7b",
   "metadata": {},
   "source": [
    "I setup some initial global parameters, such as relevant links to the data sources, and my \n",
    "tokens to connect with any APIs.\n",
    "\n",
    "In particular, I use OpenDataPhilly to get the crime data (and I'll use it again for getting the \n",
    "census tract data for the Philadelphia region). I use NOAA to get weather data for each data from a\n",
    "specific weather station, which is the Philadelphia International Airport. Finally, I use the \n",
    "official U.S. Census API to get census data.\n",
    "\n",
    "*Note that ideally, I originally wanted to retrieve weather data more accurately, since there are \n",
    "multiple weather stations in Philadelphia. I would've just retrieved the data from said stations,\n",
    "and then match each crime to the weather of the closest station. However, due to NOAA API request \n",
    "limitations for a free account, I simplified this step to get weather from one station to represent \n",
    "the entire region. Since the region is relatively small (compared to if I did this for say, the\n",
    "whole state of Pennsylvania), this should still be relatively accurate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c24cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokens:\n",
    "# NOTE: Actual key is hidden for security reasons, but you can use the following links to get your\n",
    "# own key to connect with the APIs\n",
    "# NOAA: https://www.ncdc.noaa.gov/cdo-web/token\n",
    "# Census: https://api.census.gov/data/key_signup.html\n",
    "NOAA_TOKEN = \"\"\n",
    "CENSUS_TOKEN = \"\"\n",
    "\n",
    "# Define crime data sources\n",
    "CARTO_URL = \"https://phl.carto.com/api/v2/sql\"\n",
    "CRIME_TABLE_NAME = \"incidents_part1_part2\"\n",
    "\n",
    "# Define weather station source for NOAA API\n",
    "# TODO: Incorperate weather station data from multiple stations\n",
    "WEATHER_STATION_ID = \"GHCND:USW00013739\"  # Philadelphia Intl Airport\n",
    "WEATHER_URL = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n",
    "\n",
    "# Define census data sources\n",
    "FCC_CENSUS_URL = \"https://geo.fcc.gov/api/census/block/find\"\n",
    "# Using 2019-2023 ACS 5-Year estimates\n",
    "CENSUS_API_URL = \"https://api.census.gov/data/2023/acs/acs5\"  #\n",
    "# Philadelphia County FIPS code is 101, and the state FIPS code for Pennsylvania is 42.\n",
    "# The weather station ID for Philadelphia Intl Airport is GHCND:USW0001373\n",
    "PHILLY_COUNTY_FIPS = \"101\"\n",
    "PA_STATE_FIPS = \"42\"\n",
    "\n",
    "# The URL for the census tract shapes, which is used to map crimes to census tracts\n",
    "CENSUS_SHAPE_URL = \"https://hub.arcgis.com/api/v3/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\n",
    "\n",
    "# Define date range to collect date (using a 3-year rolling window)\n",
    "# Contains data up to yesterday, since you cannot get all the data for current date\n",
    "END_DATE = datetime.now() - timedelta(days=1)\n",
    "START_DATE = END_DATE - timedelta(days=3 * 365)\n",
    "START_STR = START_DATE.strftime(\"%Y-%m-%d\")\n",
    "END_STR = END_DATE.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Conversion factor from square meters to square kilometers\n",
    "SQ_METERS_PER_SQ_KM = 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f509a",
   "metadata": {},
   "source": [
    "Now, I get the crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8faba893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting crime data from 2022-07-08 to 2025-07-07...\n",
      "Cleaning crime data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_20132\\445612047.py:110: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  crime_df['dispatch_time'] = pd.to_datetime(crime_df['dispatch_time'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min dispatch_date: 2022-07-08 00:00:00\n",
      "Max dispatch_date: 2025-07-07 00:00:00\n",
      "  dispatch_date       dispatch_time                    address_block  \\\n",
      "0    2025-05-02 2025-07-08 23:44:00              1100 BLOCK S 4TH ST   \n",
      "1    2025-04-23 2025-07-08 18:18:00  300 BLOCK MARTIN LUTHER KING DR   \n",
      "2    2025-04-23 2025-07-08 18:32:00             1700 BLOCK N 32ND ST   \n",
      "3    2025-04-24 2025-07-08 00:47:00          1300 BLOCK W Venango St   \n",
      "4    2025-02-26 2025-07-08 20:16:00              400 BLOCK N 35TH ST   \n",
      "\n",
      "         lat        lon  district_01  district_02  district_03  district_05  \\\n",
      "0  39.934248 -75.150833        False        False         True        False   \n",
      "1  39.962990 -75.178868        False        False        False        False   \n",
      "2  39.983588 -75.186199        False        False        False        False   \n",
      "3  40.007425 -75.149843        False        False        False        False   \n",
      "4  39.961642 -75.192723        False        False        False        False   \n",
      "\n",
      "   district_06  ...  crime_Vagrancy/Loitering  \\\n",
      "0        False  ...                     False   \n",
      "1        False  ...                     False   \n",
      "2        False  ...                     False   \n",
      "3        False  ...                     False   \n",
      "4        False  ...                     False   \n",
      "\n",
      "   crime_Vandalism/Criminal Mischief  crime_Weapon Violations  \\\n",
      "0                              False                    False   \n",
      "1                              False                    False   \n",
      "2                              False                    False   \n",
      "3                              False                    False   \n",
      "4                              False                    False   \n",
      "\n",
      "   dispatch_date_dt  hour_sin      hour_cos  month_sin  month_cos  \\\n",
      "0        2025-05-02 -0.258819  9.659258e-01   0.500000  -0.866025   \n",
      "1        2025-04-23 -1.000000 -1.836970e-16   0.866025  -0.500000   \n",
      "2        2025-04-23 -1.000000 -1.836970e-16   0.866025  -0.500000   \n",
      "3        2025-04-24  0.000000  1.000000e+00   0.866025  -0.500000   \n",
      "4        2025-02-26 -0.866025  5.000000e-01   0.866025   0.500000   \n",
      "\n",
      "   day_of_week_sin  day_of_week_cos  \n",
      "0        -0.433884        -0.900969  \n",
      "1         0.974928        -0.222521  \n",
      "2         0.974928        -0.222521  \n",
      "3         0.433884        -0.900969  \n",
      "4         0.974928        -0.222521  \n",
      "\n",
      "[5 rows x 72 columns]\n",
      "(464083, 72)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 464083 entries, 0 to 481289\n",
      "Data columns (total 72 columns):\n",
      " #   Column                                         Non-Null Count   Dtype         \n",
      "---  ------                                         --------------   -----         \n",
      " 0   dispatch_date                                  464083 non-null  datetime64[ns]\n",
      " 1   dispatch_time                                  464083 non-null  datetime64[ns]\n",
      " 2   address_block                                  464054 non-null  object        \n",
      " 3   lat                                            464083 non-null  float64       \n",
      " 4   lon                                            464083 non-null  float64       \n",
      " 5   district_01                                    464083 non-null  bool          \n",
      " 6   district_02                                    464083 non-null  bool          \n",
      " 7   district_03                                    464083 non-null  bool          \n",
      " 8   district_05                                    464083 non-null  bool          \n",
      " 9   district_06                                    464083 non-null  bool          \n",
      " 10  district_07                                    464083 non-null  bool          \n",
      " 11  district_08                                    464083 non-null  bool          \n",
      " 12  district_09                                    464083 non-null  bool          \n",
      " 13  district_12                                    464083 non-null  bool          \n",
      " 14  district_14                                    464083 non-null  bool          \n",
      " 15  district_15                                    464083 non-null  bool          \n",
      " 16  district_16                                    464083 non-null  bool          \n",
      " 17  district_17                                    464083 non-null  bool          \n",
      " 18  district_18                                    464083 non-null  bool          \n",
      " 19  district_19                                    464083 non-null  bool          \n",
      " 20  district_22                                    464083 non-null  bool          \n",
      " 21  district_24                                    464083 non-null  bool          \n",
      " 22  district_25                                    464083 non-null  bool          \n",
      " 23  district_26                                    464083 non-null  bool          \n",
      " 24  district_35                                    464083 non-null  bool          \n",
      " 25  district_39                                    464083 non-null  bool          \n",
      " 26  district_77                                    464083 non-null  bool          \n",
      " 27  psa_1                                          464083 non-null  bool          \n",
      " 28  psa_2                                          464083 non-null  bool          \n",
      " 29  psa_3                                          464083 non-null  bool          \n",
      " 30  psa_4                                          464083 non-null  bool          \n",
      " 31  psa_5                                          464083 non-null  bool          \n",
      " 32  psa_A                                          464083 non-null  bool          \n",
      " 33  crime_Aggravated Assault Firearm               464083 non-null  bool          \n",
      " 34  crime_Aggravated Assault No Firearm            464083 non-null  bool          \n",
      " 35  crime_All Other Offenses                       464083 non-null  bool          \n",
      " 36  crime_Arson                                    464083 non-null  bool          \n",
      " 37  crime_Burglary Non-Residential                 464083 non-null  bool          \n",
      " 38  crime_Burglary Residential                     464083 non-null  bool          \n",
      " 39  crime_DRIVING UNDER THE INFLUENCE              464083 non-null  bool          \n",
      " 40  crime_Disorderly Conduct                       464083 non-null  bool          \n",
      " 41  crime_Embezzlement                             464083 non-null  bool          \n",
      " 42  crime_Forgery and Counterfeiting               464083 non-null  bool          \n",
      " 43  crime_Fraud                                    464083 non-null  bool          \n",
      " 44  crime_Gambling Violations                      464083 non-null  bool          \n",
      " 45  crime_Homicide - Criminal                      464083 non-null  bool          \n",
      " 46  crime_Homicide - Gross Negligence              464083 non-null  bool          \n",
      " 47  crime_Homicide - Justifiable                   464083 non-null  bool          \n",
      " 48  crime_Liquor Law Violations                    464083 non-null  bool          \n",
      " 49  crime_Motor Vehicle Theft                      464083 non-null  bool          \n",
      " 50  crime_Narcotic / Drug Law Violations           464083 non-null  bool          \n",
      " 51  crime_Offenses Against Family and Children     464083 non-null  bool          \n",
      " 52  crime_Other Assaults                           464083 non-null  bool          \n",
      " 53  crime_Other Sex Offenses (Not Commercialized)  464083 non-null  bool          \n",
      " 54  crime_Prostitution and Commercialized Vice     464083 non-null  bool          \n",
      " 55  crime_Public Drunkenness                       464083 non-null  bool          \n",
      " 56  crime_Rape                                     464083 non-null  bool          \n",
      " 57  crime_Receiving Stolen Property                464083 non-null  bool          \n",
      " 58  crime_Robbery Firearm                          464083 non-null  bool          \n",
      " 59  crime_Robbery No Firearm                       464083 non-null  bool          \n",
      " 60  crime_Theft from Vehicle                       464083 non-null  bool          \n",
      " 61  crime_Thefts                                   464083 non-null  bool          \n",
      " 62  crime_Vagrancy/Loitering                       464083 non-null  bool          \n",
      " 63  crime_Vandalism/Criminal Mischief              464083 non-null  bool          \n",
      " 64  crime_Weapon Violations                        464083 non-null  bool          \n",
      " 65  dispatch_date_dt                               464083 non-null  object        \n",
      " 66  hour_sin                                       464083 non-null  float64       \n",
      " 67  hour_cos                                       464083 non-null  float64       \n",
      " 68  month_sin                                      464083 non-null  float64       \n",
      " 69  month_cos                                      464083 non-null  float64       \n",
      " 70  day_of_week_sin                                464083 non-null  float64       \n",
      " 71  day_of_week_cos                                464083 non-null  float64       \n",
      "dtypes: bool(60), datetime64[ns](2), float64(8), object(2)\n",
      "memory usage: 72.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get and clean the crime data\n",
    "crime_df = fetch_crime_data(CRIME_TABLE_NAME, START_STR, END_STR)\n",
    "crime_df = clean_crime_data(crime_df)\n",
    "\n",
    "# Print out some basic information about the crime data as sanity check\n",
    "print(crime_df.head())\n",
    "print(crime_df.shape)\n",
    "print(crime_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a9b92",
   "metadata": {},
   "source": [
    "Next, I get the weather data. Since I have to split up my API calls to avoid errors, I have to use\n",
    "some extra code to get all the data in chunks, and then combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45df10d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing weather data year-by-year...\n",
      "-> Downloading weather for year 2022...\n",
      "Fetching weather from 2022-01-01 to 2022-12-31...\n",
      "Warning: Request failed (HTTPSConnectionPool(host='www.ncdc.noaa.gov', port=443): Read timed out. (read timeout=20)). Retrying in 1s...\n",
      "-> Downloading weather for year 2023...\n",
      "Fetching weather from 2023-01-01 to 2023-12-31...\n",
      "-> Downloading weather for year 2024...\n",
      "Fetching weather from 2024-01-01 to 2024-12-31...\n",
      "Warning: Request failed (HTTPSConnectionPool(host='www.ncdc.noaa.gov', port=443): Read timed out. (read timeout=20)). Retrying in 1s...\n",
      "-> Downloading weather for year 2025...\n",
      "Fetching weather from 2025-01-01 to 2025-12-31...\n",
      "\n",
      " Successfully combined weather data for 1094 days.\n",
      "datatype     date_dt  avg_wind_speed_mph  precipitation_inches  \\\n",
      "188       2022-07-08                 6.0                  0.00   \n",
      "189       2022-07-09                 3.6                  0.02   \n",
      "190       2022-07-10                 5.8                  0.00   \n",
      "191       2022-07-11                 6.7                  0.00   \n",
      "192       2022-07-12                12.8                  0.00   \n",
      "\n",
      "datatype  snowfall_inches  snow_depth_inches  max_temp_f  min_temp_f  \n",
      "188                   0.0                0.0        88.0        68.0  \n",
      "189                   0.0                0.0        78.0        70.0  \n",
      "190                   0.0                0.0        83.0        68.0  \n",
      "191                   0.0                0.0        86.0        65.0  \n",
      "192                   0.0                0.0        93.0        70.0  \n",
      "(1094, 7)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1094 entries, 188 to 1281\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   date_dt               1094 non-null   object \n",
      " 1   avg_wind_speed_mph    1088 non-null   float64\n",
      " 2   precipitation_inches  1094 non-null   float64\n",
      " 3   snowfall_inches       1092 non-null   float64\n",
      " 4   snow_depth_inches     1092 non-null   float64\n",
      " 5   max_temp_f            1094 non-null   float64\n",
      " 6   min_temp_f            1094 non-null   float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 68.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get weather data\n",
    "print(\"Processing weather data year-by-year...\")\n",
    "\n",
    "# Initialize an empty list to hold the DataFrames for each year\n",
    "weather_frames = []\n",
    "# Calculate the years within your 3-year date range\n",
    "start_year = START_DATE.year\n",
    "end_year = END_DATE.year\n",
    "\n",
    "# Loop through each year in the range and fetch the weather data\n",
    "for year in range(start_year, end_year + 1):\n",
    "    # Define the start and end for this specific year's API call\n",
    "    year_start_str = f\"{year}-01-01\"\n",
    "    year_end_str = f\"{year}-12-31\"\n",
    "    print(f\"-> Downloading weather for year {year}...\")\n",
    "    df_year = fetch_weather_data(\n",
    "        WEATHER_STATION_ID, year_start_str, year_end_str, NOAA_TOKEN\n",
    "    )\n",
    "\n",
    "    if not df_year.empty:\n",
    "        weather_frames.append(df_year)\n",
    "\n",
    "# If I successfully downloaded data for any year, combine them into a single DataFrame\n",
    "if weather_frames:\n",
    "    # Combine the data from all successful yearly calls\n",
    "    weather_df = pd.concat(weather_frames, ignore_index=True)\n",
    "    # Filter the combined data to the precise 3-year rolling window\n",
    "    weather_df[\"date_dt_obj\"] = pd.to_datetime(weather_df[\"date_dt\"])\n",
    "    weather_df = weather_df[\n",
    "        (weather_df[\"date_dt_obj\"] >= pd.to_datetime(START_STR))\n",
    "        & (weather_df[\"date_dt_obj\"] <= pd.to_datetime(END_STR))\n",
    "    ].drop(columns=[\"date_dt_obj\"])\n",
    "    print(f\"\\n Successfully combined weather data for {len(weather_df)} days.\")\n",
    "else:\n",
    "    print(\"\\n No weather data could be downloaded.\")\n",
    "    weather_df = pd.DataFrame()\n",
    "\n",
    "weather_df = clean_weather_data(weather_df)\n",
    "\n",
    "# Print out some basic information about the weather data as sanity check\n",
    "print(weather_df.head())\n",
    "print(weather_df.shape)\n",
    "print(weather_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0731e6",
   "metadata": {},
   "source": [
    "Now, I get the census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ae92c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading census data for Philadelphia County (FIPS: 42101)...\n",
      "Downloaded and processed census data for 408 tracts.\n",
      "    tract_fips  pop_total  income_median  median_age  poverty_rate  \\\n",
      "0  42101000101     1996.0         108438        32.1      0.016032   \n",
      "1  42101000102     3025.0         108203        33.0      0.058182   \n",
      "2  42101000200     3259.0          97256        38.9      0.235655   \n",
      "3  42101000300     4236.0         102330        34.8      0.064684   \n",
      "4  42101000401     2857.0          89663        33.7      0.178509   \n",
      "\n",
      "   vacancy_rate  renter_occupancy_rate  \n",
      "0      0.067500               0.746649  \n",
      "1      0.100044               0.622725  \n",
      "2      0.074881               0.535017  \n",
      "3      0.064710               0.808845  \n",
      "4      0.117572               0.683983  \n",
      "(408, 7)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 408 entries, 0 to 407\n",
      "Data columns (total 7 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   tract_fips             408 non-null    object \n",
      " 1   pop_total              391 non-null    float64\n",
      " 2   income_median          408 non-null    int64  \n",
      " 3   median_age             408 non-null    float64\n",
      " 4   poverty_rate           408 non-null    float64\n",
      " 5   vacancy_rate           408 non-null    float64\n",
      " 6   renter_occupancy_rate  408 non-null    float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 22.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get and clean census data\n",
    "census_df = fetch_census_data(PA_STATE_FIPS, PHILLY_COUNTY_FIPS, CENSUS_TOKEN)\n",
    "census_df = clean_census_data(census_df)\n",
    "\n",
    "# Print out some basic information about the census data as sanity check\n",
    "print(census_df.head())\n",
    "print(census_df.shape)\n",
    "print(census_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fe073",
   "metadata": {},
   "source": [
    "At this point, the most important parts of the data are stored. However, to properly map the census\n",
    "data to the crime data, I need to determine which census tract each crime is located in.\n",
    "\n",
    "To do this, I need to get the geojson file from OpenDataPhilly, which is a file that effectively \n",
    "outlines each census tract in the city of Philadelphia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3234ec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching census tracts from OpenDataPhilly API...\n",
      "                                            geometry  OBJECTID STATEFP10  \\\n",
      "0  POLYGON ((-75.22927 39.96054, -75.22865 39.960...         1        42   \n",
      "1  POLYGON ((-75.23536 39.96852, -75.23545 39.969...         2        42   \n",
      "2  POLYGON ((-75.24343 39.9623, -75.24339 39.9622...         3        42   \n",
      "3  POLYGON ((-75.17341 39.97779, -75.17386 39.977...         4        42   \n",
      "4  POLYGON ((-75.17313 39.97776, -75.17321 39.977...         5        42   \n",
      "\n",
      "  COUNTYFP10 TRACTCE10      GEOID10 NAME10        NAMELSAD10 MTFCC10  \\\n",
      "0        101    009400  42101009400     94   Census Tract 94   G5020   \n",
      "1        101    009500  42101009500     95   Census Tract 95   G5020   \n",
      "2        101    009600  42101009600     96   Census Tract 96   G5020   \n",
      "3        101    013800  42101013800    138  Census Tract 138   G5020   \n",
      "4        101    013900  42101013900    139  Census Tract 139   G5020   \n",
      "\n",
      "  FUNCSTAT10  ALAND10  AWATER10   INTPTLAT10    INTPTLON10 LOGRECNO  \n",
      "0          S   366717         0  +39.9632709  -075.2322437    10429  \n",
      "1          S   319070         0  +39.9658709  -075.2379140    10430  \n",
      "2          S   405273         0  +39.9655396  -075.2435075    10431  \n",
      "3          S   341256         0  +39.9764504  -075.1771771    10468  \n",
      "4          S   562934         0  +39.9750563  -075.1711846    10469  \n"
     ]
    }
   ],
   "source": [
    "# Get Census Tract geojson from OpenDataPhilly, to help map crime locations to census tracts\n",
    "# TODO: This currently retrieves tracts from 2010, since that's the most recent option available.\n",
    "# Of course, this is not ideal, and should be switched to 2020 when available.\n",
    "gdf_tracts = get_census_tracts()\n",
    "\n",
    "# Print out some basic information about the census tracts as sanity check\n",
    "print(gdf_tracts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535af39",
   "metadata": {},
   "source": [
    "This gives a lot of information for each census tract. Though, it has a lot of columns that is not\n",
    "neccessary. Regardless, my next step is to convert my crime data into a geo-dataframe, allowing me\n",
    "to merge it with the census tract with a spatial join. I also clean up the data to remove a lot of \n",
    "the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3146c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting crime data to GeoDataFrame...\n",
      "Performing spatial join to map crimes to census tracts...\n",
      "\n",
      " Spatial join complete. Crime data now includes census tract info.\n",
      "  dispatch_date       dispatch_time                    address_block  \\\n",
      "0    2025-05-02 2025-07-08 23:44:00              1100 BLOCK S 4TH ST   \n",
      "1    2025-04-23 2025-07-08 18:18:00  300 BLOCK MARTIN LUTHER KING DR   \n",
      "2    2025-04-23 2025-07-08 18:32:00             1700 BLOCK N 32ND ST   \n",
      "3    2025-04-24 2025-07-08 00:47:00          1300 BLOCK W Venango St   \n",
      "4    2025-02-26 2025-07-08 20:16:00              400 BLOCK N 35TH ST   \n",
      "\n",
      "         lat        lon  district_01  district_02  district_03  district_05  \\\n",
      "0  39.934248 -75.150833        False        False         True        False   \n",
      "1  39.962990 -75.178868        False        False        False        False   \n",
      "2  39.983588 -75.186199        False        False        False        False   \n",
      "3  40.007425 -75.149843        False        False        False        False   \n",
      "4  39.961642 -75.192723        False        False        False        False   \n",
      "\n",
      "   district_06  ...  dispatch_date_dt  hour_sin      hour_cos  month_sin  \\\n",
      "0        False  ...        2025-05-02 -0.258819  9.659258e-01   0.500000   \n",
      "1        False  ...        2025-04-23 -1.000000 -1.836970e-16   0.866025   \n",
      "2        False  ...        2025-04-23 -1.000000 -1.836970e-16   0.866025   \n",
      "3        False  ...        2025-04-24  0.000000  1.000000e+00   0.866025   \n",
      "4        False  ...        2025-02-26 -0.866025  5.000000e-01   0.866025   \n",
      "\n",
      "   month_cos  day_of_week_sin  day_of_week_cos                    geometry  \\\n",
      "0  -0.866025        -0.433884        -0.900969  POINT (-75.15083 39.93425)   \n",
      "1  -0.500000         0.974928        -0.222521  POINT (-75.17887 39.96299)   \n",
      "2  -0.500000         0.974928        -0.222521   POINT (-75.1862 39.98359)   \n",
      "3  -0.500000         0.433884        -0.900969  POINT (-75.14984 40.00742)   \n",
      "4   0.500000         0.974928        -0.222521  POINT (-75.19272 39.96164)   \n",
      "\n",
      "      tract_id  land_area_sq_meters  \n",
      "0  42101002500               398697  \n",
      "1  42101012500               864810  \n",
      "2  42101014900               453221  \n",
      "3  42101020300               508176  \n",
      "4  42101009000               436319  \n",
      "\n",
      "[5 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "# Map crime locations to census tracts using spatial joins\n",
    "final_crime_data = merge_crime_census(crime_df, gdf_tracts)\n",
    "\n",
    "# Display the result\n",
    "print(final_crime_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06966836",
   "metadata": {},
   "source": [
    "Now, all three datasets are ready to be merged together into one big dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa435f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging crime and weather data...\n",
      "Merging with census data...\n",
      "\n",
      " Merge complete!\n",
      "Columns in the final dataset:\n",
      "Index(['dispatch_date', 'dispatch_time', 'address_block', 'lat', 'lon',\n",
      "       'district_01', 'district_02', 'district_03', 'district_05',\n",
      "       'district_06', 'district_07', 'district_08', 'district_09',\n",
      "       'district_12', 'district_14', 'district_15', 'district_16',\n",
      "       'district_17', 'district_18', 'district_19', 'district_22',\n",
      "       'district_24', 'district_25', 'district_26', 'district_35',\n",
      "       'district_39', 'district_77', 'psa_1', 'psa_2', 'psa_3', 'psa_4',\n",
      "       'psa_5', 'psa_A', 'crime_Aggravated Assault Firearm',\n",
      "       'crime_Aggravated Assault No Firearm', 'crime_All Other Offenses',\n",
      "       'crime_Arson', 'crime_Burglary Non-Residential',\n",
      "       'crime_Burglary Residential', 'crime_DRIVING UNDER THE INFLUENCE',\n",
      "       'crime_Disorderly Conduct', 'crime_Embezzlement',\n",
      "       'crime_Forgery and Counterfeiting', 'crime_Fraud',\n",
      "       'crime_Gambling Violations', 'crime_Homicide - Criminal',\n",
      "       'crime_Homicide - Gross Negligence', 'crime_Homicide - Justifiable',\n",
      "       'crime_Liquor Law Violations', 'crime_Motor Vehicle Theft',\n",
      "       'crime_Narcotic / Drug Law Violations',\n",
      "       'crime_Offenses Against Family and Children', 'crime_Other Assaults',\n",
      "       'crime_Other Sex Offenses (Not Commercialized)',\n",
      "       'crime_Prostitution and Commercialized Vice',\n",
      "       'crime_Public Drunkenness', 'crime_Rape',\n",
      "       'crime_Receiving Stolen Property', 'crime_Robbery Firearm',\n",
      "       'crime_Robbery No Firearm', 'crime_Theft from Vehicle', 'crime_Thefts',\n",
      "       'crime_Vagrancy/Loitering', 'crime_Vandalism/Criminal Mischief',\n",
      "       'crime_Weapon Violations', 'dispatch_date_dt', 'hour_sin', 'hour_cos',\n",
      "       'month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
      "       'geometry', 'tract_id', 'land_area_sq_meters', 'avg_wind_speed_mph',\n",
      "       'precipitation_inches', 'snowfall_inches', 'snow_depth_inches',\n",
      "       'max_temp_f', 'min_temp_f', 'pop_total', 'income_median', 'median_age',\n",
      "       'poverty_rate', 'vacancy_rate', 'renter_occupancy_rate'],\n",
      "      dtype='object')\n",
      "\n",
      "Sample of the fully merged data:\n",
      "  dispatch_date       dispatch_time            address_block        lat  \\\n",
      "0    2025-05-02 2025-07-08 23:44:00      1100 BLOCK S 4TH ST  39.934248   \n",
      "2    2025-04-23 2025-07-08 18:32:00     1700 BLOCK N 32ND ST  39.983588   \n",
      "3    2025-04-24 2025-07-08 00:47:00  1300 BLOCK W Venango St  40.007425   \n",
      "4    2025-02-26 2025-07-08 20:16:00      400 BLOCK N 35TH ST  39.961642   \n",
      "5    2025-04-13 2025-07-08 03:15:00      5100 BLOCK N 5TH ST  40.029310   \n",
      "\n",
      "         lon  district_01  district_02  district_03  district_05  district_06  \\\n",
      "0 -75.150833        False        False         True        False        False   \n",
      "2 -75.186199        False        False        False        False        False   \n",
      "3 -75.149843        False        False        False        False        False   \n",
      "4 -75.192723        False        False        False        False        False   \n",
      "5 -75.132056        False        False        False        False        False   \n",
      "\n",
      "   ...  snowfall_inches  snow_depth_inches  max_temp_f  min_temp_f  pop_total  \\\n",
      "0  ...              0.0                0.0        86.0        59.0     4306.0   \n",
      "2  ...              0.0                0.0        77.0        54.0     4140.0   \n",
      "3  ...              0.0                0.0        79.0        50.0     2807.0   \n",
      "4  ...              0.0                0.0        61.0        35.0     7525.0   \n",
      "5  ...              0.0                0.0        62.0        42.0     7170.0   \n",
      "\n",
      "   income_median  median_age  poverty_rate  vacancy_rate  \\\n",
      "0        83508.0        36.4      0.204366      0.032818   \n",
      "2        37431.0        35.5      0.401691      0.130470   \n",
      "3        49115.0        44.4      0.259708      0.204270   \n",
      "4        38976.0        20.0      0.205050      0.104931   \n",
      "5        60892.0        34.2      0.217434      0.042004   \n",
      "\n",
      "   renter_occupancy_rate  \n",
      "0               0.440136  \n",
      "2               0.550792  \n",
      "3               0.464222  \n",
      "4               0.880205  \n",
      "5               0.310769  \n",
      "\n",
      "[5 rows x 87 columns]\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 417728 entries, 0 to 463369\n",
      "Data columns (total 87 columns):\n",
      " #   Column                                         Non-Null Count   Dtype         \n",
      "---  ------                                         --------------   -----         \n",
      " 0   dispatch_date                                  417728 non-null  datetime64[ns]\n",
      " 1   dispatch_time                                  417728 non-null  datetime64[ns]\n",
      " 2   address_block                                  417699 non-null  object        \n",
      " 3   lat                                            417728 non-null  float64       \n",
      " 4   lon                                            417728 non-null  float64       \n",
      " 5   district_01                                    417728 non-null  bool          \n",
      " 6   district_02                                    417728 non-null  bool          \n",
      " 7   district_03                                    417728 non-null  bool          \n",
      " 8   district_05                                    417728 non-null  bool          \n",
      " 9   district_06                                    417728 non-null  bool          \n",
      " 10  district_07                                    417728 non-null  bool          \n",
      " 11  district_08                                    417728 non-null  bool          \n",
      " 12  district_09                                    417728 non-null  bool          \n",
      " 13  district_12                                    417728 non-null  bool          \n",
      " 14  district_14                                    417728 non-null  bool          \n",
      " 15  district_15                                    417728 non-null  bool          \n",
      " 16  district_16                                    417728 non-null  bool          \n",
      " 17  district_17                                    417728 non-null  bool          \n",
      " 18  district_18                                    417728 non-null  bool          \n",
      " 19  district_19                                    417728 non-null  bool          \n",
      " 20  district_22                                    417728 non-null  bool          \n",
      " 21  district_24                                    417728 non-null  bool          \n",
      " 22  district_25                                    417728 non-null  bool          \n",
      " 23  district_26                                    417728 non-null  bool          \n",
      " 24  district_35                                    417728 non-null  bool          \n",
      " 25  district_39                                    417728 non-null  bool          \n",
      " 26  district_77                                    417728 non-null  bool          \n",
      " 27  psa_1                                          417728 non-null  bool          \n",
      " 28  psa_2                                          417728 non-null  bool          \n",
      " 29  psa_3                                          417728 non-null  bool          \n",
      " 30  psa_4                                          417728 non-null  bool          \n",
      " 31  psa_5                                          417728 non-null  bool          \n",
      " 32  psa_A                                          417728 non-null  bool          \n",
      " 33  crime_Aggravated Assault Firearm               417728 non-null  bool          \n",
      " 34  crime_Aggravated Assault No Firearm            417728 non-null  bool          \n",
      " 35  crime_All Other Offenses                       417728 non-null  bool          \n",
      " 36  crime_Arson                                    417728 non-null  bool          \n",
      " 37  crime_Burglary Non-Residential                 417728 non-null  bool          \n",
      " 38  crime_Burglary Residential                     417728 non-null  bool          \n",
      " 39  crime_DRIVING UNDER THE INFLUENCE              417728 non-null  bool          \n",
      " 40  crime_Disorderly Conduct                       417728 non-null  bool          \n",
      " 41  crime_Embezzlement                             417728 non-null  bool          \n",
      " 42  crime_Forgery and Counterfeiting               417728 non-null  bool          \n",
      " 43  crime_Fraud                                    417728 non-null  bool          \n",
      " 44  crime_Gambling Violations                      417728 non-null  bool          \n",
      " 45  crime_Homicide - Criminal                      417728 non-null  bool          \n",
      " 46  crime_Homicide - Gross Negligence              417728 non-null  bool          \n",
      " 47  crime_Homicide - Justifiable                   417728 non-null  bool          \n",
      " 48  crime_Liquor Law Violations                    417728 non-null  bool          \n",
      " 49  crime_Motor Vehicle Theft                      417728 non-null  bool          \n",
      " 50  crime_Narcotic / Drug Law Violations           417728 non-null  bool          \n",
      " 51  crime_Offenses Against Family and Children     417728 non-null  bool          \n",
      " 52  crime_Other Assaults                           417728 non-null  bool          \n",
      " 53  crime_Other Sex Offenses (Not Commercialized)  417728 non-null  bool          \n",
      " 54  crime_Prostitution and Commercialized Vice     417728 non-null  bool          \n",
      " 55  crime_Public Drunkenness                       417728 non-null  bool          \n",
      " 56  crime_Rape                                     417728 non-null  bool          \n",
      " 57  crime_Receiving Stolen Property                417728 non-null  bool          \n",
      " 58  crime_Robbery Firearm                          417728 non-null  bool          \n",
      " 59  crime_Robbery No Firearm                       417728 non-null  bool          \n",
      " 60  crime_Theft from Vehicle                       417728 non-null  bool          \n",
      " 61  crime_Thefts                                   417728 non-null  bool          \n",
      " 62  crime_Vagrancy/Loitering                       417728 non-null  bool          \n",
      " 63  crime_Vandalism/Criminal Mischief              417728 non-null  bool          \n",
      " 64  crime_Weapon Violations                        417728 non-null  bool          \n",
      " 65  dispatch_date_dt                               417728 non-null  object        \n",
      " 66  hour_sin                                       417728 non-null  float64       \n",
      " 67  hour_cos                                       417728 non-null  float64       \n",
      " 68  month_sin                                      417728 non-null  float64       \n",
      " 69  month_cos                                      417728 non-null  float64       \n",
      " 70  day_of_week_sin                                417728 non-null  float64       \n",
      " 71  day_of_week_cos                                417728 non-null  float64       \n",
      " 72  geometry                                       417728 non-null  geometry      \n",
      " 73  tract_id                                       417728 non-null  object        \n",
      " 74  land_area_sq_meters                            417728 non-null  int64         \n",
      " 75  avg_wind_speed_mph                             415122 non-null  float64       \n",
      " 76  precipitation_inches                           417040 non-null  float64       \n",
      " 77  snowfall_inches                                416344 non-null  float64       \n",
      " 78  snow_depth_inches                              416303 non-null  float64       \n",
      " 79  max_temp_f                                     417040 non-null  float64       \n",
      " 80  min_temp_f                                     417040 non-null  float64       \n",
      " 81  pop_total                                      417728 non-null  float64       \n",
      " 82  income_median                                  417728 non-null  float64       \n",
      " 83  median_age                                     417728 non-null  float64       \n",
      " 84  poverty_rate                                   417728 non-null  float64       \n",
      " 85  vacancy_rate                                   417728 non-null  float64       \n",
      " 86  renter_occupancy_rate                          417728 non-null  float64       \n",
      "dtypes: bool(60), datetime64[ns](2), float64(20), geometry(1), int64(1), object(3)\n",
      "memory usage: 113.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging crime and weather data...\")\n",
    "crime_and_weather_df = pd.merge(\n",
    "    final_crime_data,\n",
    "    weather_df,\n",
    "    left_on=\"dispatch_date_dt\",\n",
    "    right_on=\"date_dt\",\n",
    "    how=\"left\",\n",
    ")\n",
    "print(\"Merging with census data...\")\n",
    "merged_df = pd.merge(\n",
    "    crime_and_weather_df,\n",
    "    census_df,\n",
    "    left_on=\"tract_id\",\n",
    "    right_on=\"tract_fips\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Clean up by dropping redundant key columns\n",
    "merged_df = merged_df.drop(columns=[\"date_dt\", \"tract_fips\"], errors=\"ignore\")\n",
    "merged_df = merged_df.dropna(subset=[\"tract_id\", \"pop_total\"])\n",
    "\n",
    "# As always, print out some basic information about the final merged dataset for sanity check\n",
    "print(\"\\n Merge complete!\")\n",
    "print(\"Columns in the final dataset:\")\n",
    "print(merged_df.columns)\n",
    "print(\"\\nSample of the fully merged data:\")\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660b710",
   "metadata": {},
   "source": [
    "As one final step, I go ahead and add a population density column. This can be important for the\n",
    "clustering, since crime certainly might appear more often/less often based on how many people are \n",
    "in a local census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6281af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating population density...\n",
      "Population density calculated.\n",
      "      tract_id  pop_total  land_area_sq_meters  pop_density_sq_km\n",
      "0  42101002500     4306.0               398697       10800.181592\n",
      "2  42101014900     4140.0               453221        9134.616445\n",
      "3  42101020300     2807.0               508176        5523.676836\n",
      "4  42101009000     7525.0               436319       17246.555846\n",
      "5  42101028600     7170.0               717845        9988.228657\n"
     ]
    }
   ],
   "source": [
    "merged_df = calculate_population_density(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5c28b",
   "metadata": {},
   "source": [
    "Now, I just remove any rows with missing rows, since we cannot have missing values for clustering.\n",
    "At this point, these are likely due to user error and make up the very small minority of our \n",
    "dataset, so I don't go through the trouble to try and impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a856d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 413660 entries, 0 to 463369\n",
      "Data columns (total 88 columns):\n",
      " #   Column                                         Non-Null Count   Dtype         \n",
      "---  ------                                         --------------   -----         \n",
      " 0   dispatch_date                                  413660 non-null  datetime64[ns]\n",
      " 1   dispatch_time                                  413660 non-null  datetime64[ns]\n",
      " 2   address_block                                  413660 non-null  object        \n",
      " 3   lat                                            413660 non-null  float64       \n",
      " 4   lon                                            413660 non-null  float64       \n",
      " 5   district_01                                    413660 non-null  bool          \n",
      " 6   district_02                                    413660 non-null  bool          \n",
      " 7   district_03                                    413660 non-null  bool          \n",
      " 8   district_05                                    413660 non-null  bool          \n",
      " 9   district_06                                    413660 non-null  bool          \n",
      " 10  district_07                                    413660 non-null  bool          \n",
      " 11  district_08                                    413660 non-null  bool          \n",
      " 12  district_09                                    413660 non-null  bool          \n",
      " 13  district_12                                    413660 non-null  bool          \n",
      " 14  district_14                                    413660 non-null  bool          \n",
      " 15  district_15                                    413660 non-null  bool          \n",
      " 16  district_16                                    413660 non-null  bool          \n",
      " 17  district_17                                    413660 non-null  bool          \n",
      " 18  district_18                                    413660 non-null  bool          \n",
      " 19  district_19                                    413660 non-null  bool          \n",
      " 20  district_22                                    413660 non-null  bool          \n",
      " 21  district_24                                    413660 non-null  bool          \n",
      " 22  district_25                                    413660 non-null  bool          \n",
      " 23  district_26                                    413660 non-null  bool          \n",
      " 24  district_35                                    413660 non-null  bool          \n",
      " 25  district_39                                    413660 non-null  bool          \n",
      " 26  district_77                                    413660 non-null  bool          \n",
      " 27  psa_1                                          413660 non-null  bool          \n",
      " 28  psa_2                                          413660 non-null  bool          \n",
      " 29  psa_3                                          413660 non-null  bool          \n",
      " 30  psa_4                                          413660 non-null  bool          \n",
      " 31  psa_5                                          413660 non-null  bool          \n",
      " 32  psa_A                                          413660 non-null  bool          \n",
      " 33  crime_Aggravated Assault Firearm               413660 non-null  bool          \n",
      " 34  crime_Aggravated Assault No Firearm            413660 non-null  bool          \n",
      " 35  crime_All Other Offenses                       413660 non-null  bool          \n",
      " 36  crime_Arson                                    413660 non-null  bool          \n",
      " 37  crime_Burglary Non-Residential                 413660 non-null  bool          \n",
      " 38  crime_Burglary Residential                     413660 non-null  bool          \n",
      " 39  crime_DRIVING UNDER THE INFLUENCE              413660 non-null  bool          \n",
      " 40  crime_Disorderly Conduct                       413660 non-null  bool          \n",
      " 41  crime_Embezzlement                             413660 non-null  bool          \n",
      " 42  crime_Forgery and Counterfeiting               413660 non-null  bool          \n",
      " 43  crime_Fraud                                    413660 non-null  bool          \n",
      " 44  crime_Gambling Violations                      413660 non-null  bool          \n",
      " 45  crime_Homicide - Criminal                      413660 non-null  bool          \n",
      " 46  crime_Homicide - Gross Negligence              413660 non-null  bool          \n",
      " 47  crime_Homicide - Justifiable                   413660 non-null  bool          \n",
      " 48  crime_Liquor Law Violations                    413660 non-null  bool          \n",
      " 49  crime_Motor Vehicle Theft                      413660 non-null  bool          \n",
      " 50  crime_Narcotic / Drug Law Violations           413660 non-null  bool          \n",
      " 51  crime_Offenses Against Family and Children     413660 non-null  bool          \n",
      " 52  crime_Other Assaults                           413660 non-null  bool          \n",
      " 53  crime_Other Sex Offenses (Not Commercialized)  413660 non-null  bool          \n",
      " 54  crime_Prostitution and Commercialized Vice     413660 non-null  bool          \n",
      " 55  crime_Public Drunkenness                       413660 non-null  bool          \n",
      " 56  crime_Rape                                     413660 non-null  bool          \n",
      " 57  crime_Receiving Stolen Property                413660 non-null  bool          \n",
      " 58  crime_Robbery Firearm                          413660 non-null  bool          \n",
      " 59  crime_Robbery No Firearm                       413660 non-null  bool          \n",
      " 60  crime_Theft from Vehicle                       413660 non-null  bool          \n",
      " 61  crime_Thefts                                   413660 non-null  bool          \n",
      " 62  crime_Vagrancy/Loitering                       413660 non-null  bool          \n",
      " 63  crime_Vandalism/Criminal Mischief              413660 non-null  bool          \n",
      " 64  crime_Weapon Violations                        413660 non-null  bool          \n",
      " 65  dispatch_date_dt                               413660 non-null  object        \n",
      " 66  hour_sin                                       413660 non-null  float64       \n",
      " 67  hour_cos                                       413660 non-null  float64       \n",
      " 68  month_sin                                      413660 non-null  float64       \n",
      " 69  month_cos                                      413660 non-null  float64       \n",
      " 70  day_of_week_sin                                413660 non-null  float64       \n",
      " 71  day_of_week_cos                                413660 non-null  float64       \n",
      " 72  geometry                                       413660 non-null  geometry      \n",
      " 73  tract_id                                       413660 non-null  object        \n",
      " 74  avg_wind_speed_mph                             413660 non-null  float64       \n",
      " 75  precipitation_inches                           413660 non-null  float64       \n",
      " 76  snowfall_inches                                413660 non-null  float64       \n",
      " 77  snow_depth_inches                              413660 non-null  float64       \n",
      " 78  max_temp_f                                     413660 non-null  float64       \n",
      " 79  min_temp_f                                     413660 non-null  float64       \n",
      " 80  pop_total                                      413660 non-null  float64       \n",
      " 81  income_median                                  413660 non-null  float64       \n",
      " 82  median_age                                     413660 non-null  float64       \n",
      " 83  poverty_rate                                   413660 non-null  float64       \n",
      " 84  vacancy_rate                                   413660 non-null  float64       \n",
      " 85  renter_occupancy_rate                          413660 non-null  float64       \n",
      " 86  area_sq_km                                     413660 non-null  float64       \n",
      " 87  pop_density_sq_km                              413660 non-null  float64       \n",
      "dtypes: bool(60), datetime64[ns](2), float64(22), geometry(1), object(3)\n",
      "memory usage: 115.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Drop any remaining missing values; these are just due to user error and are rare cases, and are\n",
    "# not worth the trouble of trying to fill in\n",
    "final_merged_df = merged_df.dropna()\n",
    "assert final_merged_df.isna().sum().sum() == 0, \"Error: Missing values were found!\"\n",
    "\n",
    "# Print out the final merged DataFrame information\n",
    "final_merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d4503",
   "metadata": {},
   "source": [
    "For further experimentation, I save the data as a static pickle file. This will be used in the next\n",
    "experimental notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e8e12ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final merged data saved to data/merged_data_2022-07-08_to_2025-07-07.pkl.\n"
     ]
    }
   ],
   "source": [
    "filepath = f\"data/merged_data_{START_STR}_to_{END_STR}.pkl\"\n",
    "final_merged_df.to_pickle(filepath)\n",
    "print(f\"\\nFinal merged data saved to {filepath}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ab1d4",
   "metadata": {},
   "source": [
    "This is just a code cell to remind me to ensure my token strings are blank before commiting, since\n",
    "it is meant to be private and commiting it would be a security concern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e99da671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mMAKE SURE TO SET NOAA AND CENSUS TOKEN TO BE EMPTY BEFORE COMMITING!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "BOLD = \"\\033[1m\"\n",
    "RED = \"\\033[31m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "# Print the bold red text\n",
    "print(\n",
    "    f\"{BOLD}{RED}MAKE SURE TO SET NOAA AND CENSUS TOKEN TO BE EMPTY BEFORE COMMITING!{RESET}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
